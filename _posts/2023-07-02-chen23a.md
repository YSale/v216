---
abstract: Off-policy learning is a key to extend reinforcement learning as it allows
  to learn  a target policy from a different behavior policy that generates the data.
  However, it is well known as “the deadly triad” when combined with bootstrapping
  and function approximation. Retrace is an efficient and  convergent off-policy algorithm
  with tabular value functions which employs  truncated importance sampling ratios.
  Unfortunately, Retrace is known to be unstable with linear function approximation.
  In this paper, we propose modified Retrace  to correct the  off-policy return, derive
  a new off-policy temporal difference learning algorithm (TD-MRetrace) with linear
  function approximation, and obtain a convergence guarantee under standard assumptions.
  Experimental results on counterexamples and control tasks validate the effectiveness
  of the proposed algorithm compared with traditional algorithms.
openreview: EBMEBLb4p3
title: Modified Retrace for Off-Policy Temporal Difference Learning
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23a
month: 0
tex_title: Modified Retrace for Off-Policy Temporal Difference Learning
firstpage: 303
lastpage: 312
page: 303-312
order: 303
cycles: false
bibtex_author: Chen, Xingguo and Ma, Xingzhou and Li, Yang and Yang, Guang and Yang,
  Shangdong and Gao, Yang
author:
- given: Xingguo
  family: Chen
- given: Xingzhou
  family: Ma
- given: Yang
  family: Li
- given: Guang
  family: Yang
- given: Shangdong
  family: Yang
- given: Yang
  family: Gao
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/chen23a/chen23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
