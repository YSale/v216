---
abstract: Learning is crucial for automated negotiation, and recent years have witnessed
  a remarkable achievement in application of reinforcement learning (RL) for various
  negotiation tasks. Conventional RL methods focus generally on learning from active
  interactions with opposing negotiators. However, collecting online data is expensive
  in many realistic negotiation scenarios. While previous studies partially mitigate
  this problem through the use of opponent simulators (i.e., agents following known
  strategies), in reality it is usually hard to fully capture an opponent’s negotiation
  strategy. Moreover, a further challenge lies in an agent’s capability of adapting
  to dynamic variations of an opponent’s preferences or strategies, which may happen
  from time to time for different reasons in subsequent negotiations. In response
  to these challenges, this article proposes a novel Deep Offline Reinforcement learning
  Negotiating Agent framework that allows to learn an effective strategy using previously
  collected negotiation datasets without requiring interaction with an opponent. This
  is in contrast to existing RL-based negotiation approaches that all rely on active
  interaction with opponents. Furthermore, the strategy fine-tuning mechanism is included
  to adjust the learned strategy in response to the preferences or strategy changes
  of the opponent. The performance of the proposed framework is evaluated based on
  a diverse set of state-of-the-art baselines under different settings. Experimental
  results show that the framework allows to learn effective strategies exclusively
  with offline datasets, and is also capable of effectively adapting to changes of
  an opponent’s preferences or strategy.
openreview: "-4t-Uvb4_FT"
title: An effective negotiating agent framework based on deep offline reinforcement
  learning
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23c
month: 0
tex_title: An effective negotiating agent framework based on deep offline reinforcement
  learning
firstpage: 324
lastpage: 335
page: 324-335
order: 324
cycles: false
bibtex_author: Chen, Siqi and Zhao, Jianing and Weiss, Gerhard and Su, Ran and Lei,
  Kaiyou
author:
- given: Siqi
  family: Chen
- given: Jianing
  family: Zhao
- given: Gerhard
  family: Weiss
- given: Ran
  family: Su
- given: Kaiyou
  family: Lei
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/chen23c/chen23c.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/chen23c/chen23c-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
