---
abstract: While many solutions for privacy-preserving convex empirical risk minimization
  (ERM) have been developed, privacy-preserving nonconvex ERM remains a challenge.
  We study nonconvex ERM, which takes the form of minimizing a finite-sum of nonconvex
  loss functions over a training set. We propose a new differentially private stochastic
  gradient descent algorithm for nonconvex ERM that achieves strong privacy guarantees
  efficiently, and provide a tight analysis of its privacy and utility guarantees,
  as well as its gradient complexity. Our algorithm reduces gradient complexity while
  matching the best-known utility guarantee. Our experiments on benchmark nonconvex
  ERM problems demonstrate superior performance in terms of both training cost and
  utility gains compared with previous differentially private methods using the same
  privacy budgets.
openreview: chL9P4wS_8e
title: Efficient Privacy-Preserving Stochastic Nonconvex Optimization
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23b
month: 0
tex_title: Efficient Privacy-Preserving Stochastic Nonconvex Optimization
firstpage: 2203
lastpage: 2213
page: 2203-2213
order: 2203
cycles: false
bibtex_author: Wang, Lingxiao and Jayaraman, Bargav and Evans, David and Gu, Quanquan
author:
- given: Lingxiao
  family: Wang
- given: Bargav
  family: Jayaraman
- given: David
  family: Evans
- given: Quanquan
  family: Gu
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/wang23b/wang23b.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/wang23b/wang23b-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
