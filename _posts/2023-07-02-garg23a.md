---
abstract: Information-theoretic clustering is one of the most promising and principled
  approaches to finding clusters with minimal apriori assumptions. The key criterion
  therein is to maximize the mutual information between the data points and their
  cluster labels. Such an approach, however, does not explicitly promote any type
  of inter-cluster behavior. We instead propose to maximize the Kullback-Leibler divergence
  between the underlying data distributions associated to clusters (referred to as
  cluster distributions). We show it to entail the mutual information criterion along
  with maximizing cross entropy between the cluster distributions. For practical efficiency,
  we propose to empirically estimate the objective of KL-D between clusters in its
  dual form leveraging deep neural nets as a dual function approximator. Remarkably,
  our theoretical analysis establishes that estimating the divergence measure in its
  dual form simplifies the problem of clustering to one of optimally finding k-1 cut
  points for k clusters in the 1-D dual functional space. Overall, our approach enables
  linear-time clustering algorithms with theoretical guarantees of near-optimality,
  owing to the submodularity of the objective. We show the empirical superiority of
  our approach w.r.t. current state-of-the-art methods on the challenging task of
  clustering noisy timeseries as observed in domains such as neuroscience, healthcare,
  financial markets, spatio-temporal environmental dynamics, etc.
openreview: GNL_L2lUK6X
title: Information theoretic clustering via divergence maximization among clusters
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: garg23a
month: 0
tex_title: Information theoretic clustering via divergence maximization among clusters
firstpage: 624
lastpage: 634
page: 624-634
order: 624
cycles: false
bibtex_author: Garg, Sahil and Dalirrooyfard, Mina and Schneider, Anderson and Adler,
  Yeshaya and Nevmyvaka, Yuriy and Chen, Yu and Li, Fengpei and Cecchi, Guillermo
author:
- given: Sahil
  family: Garg
- given: Mina
  family: Dalirrooyfard
- given: Anderson
  family: Schneider
- given: Yeshaya
  family: Adler
- given: Yuriy
  family: Nevmyvaka
- given: Yu
  family: Chen
- given: Fengpei
  family: Li
- given: Guillermo
  family: Cecchi
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/garg23a/garg23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/garg23a/garg23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
