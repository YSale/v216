---
abstract: Though Transformers have achieved promising results in many computer vision
  tasks, they tend to be over-confident in predictions, as the standard Dot Product
  Self-Attention (DPSA) can barely preserve distance for the unbounded input domain.
  In this work, we fill this gap by proposing a novel Lipschitz Regularized Transformer
  (LRFormer). Specifically, we present a new similarity function with the distance
  within Banach Space to ensure the Lipschitzness and also regularize the term by
  a contractive Lipschitz Bound. The proposed method is analyzed with a theoretical
  guarantee, providing a rigorous basis for its effectiveness and reliability. Extensive
  experiments conducted on standard vision benchmarks demonstrate that our method
  outperforms the state-of-the-art single forward pass approaches in prediction, calibration,
  and uncertainty estimation.
openreview: JF54G1_Pjo
title: Mitigating Transformer Overconfidence via Lipschitz Regularization
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ye23a
month: 0
tex_title: Mitigating Transformer Overconfidence via {L}ipschitz Regularization
firstpage: 2422
lastpage: 2432
page: 2422-2432
order: 2422
cycles: false
bibtex_author: Ye, Wenqian and Ma, Yunsheng and Cao, Xu and Tang, Kun
author:
- given: Wenqian
  family: Ye
- given: Yunsheng
  family: Ma
- given: Xu
  family: Cao
- given: Kun
  family: Tang
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/ye23a/ye23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/ye23a/ye23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
