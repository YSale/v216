---
abstract: We introduce the Kernel Calibration Conditional Stein Discrepancy test (KCCSD
  test), a nonparametric, kernel-based test for assessing the calibration of probabilistic
  models with well-defined scores. In contrast to previous methods, our test avoids
  the need for possibly expensive expectation approximations while providing control
  over its type-I error. We achieve these improvements by using a new family of kernels
  for score-based probabilities that can be estimated without probability density
  samples, and by using a Conditional Goodness of Fit criterion for the KCCSD testâ€™s
  U-statistic. We demonstrate the properties of our test on various synthetic settings.
openreview: a19hqkfUQEr
title: Fast and scalable score-based kernel calibration tests
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: glaser23a
month: 0
tex_title: Fast and scalable score-based kernel calibration tests
firstpage: 691
lastpage: 700
page: 691-700
order: 691
cycles: false
bibtex_author: Glaser, Pierre and Widmann, David and Lindsten, Fredrik and Gretton,
  Arthur
author:
- given: Pierre
  family: Glaser
- given: David
  family: Widmann
- given: Fredrik
  family: Lindsten
- given: Arthur
  family: Gretton
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/glaser23a/glaser23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/glaser23a/glaser23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
