---
abstract: Neural Processes (NPs) are appealing due to their ability to perform fast
  adaptation based on a context set. This set is encoded by a latent variable, which
  is often assumed to follow a simple distribution. However, in real-word settings,
  the context set may be drawn from richer distributions having multiple modes, heavy
  tails, etc. In this work, we provide a framework that allows NPsâ€™ latent variable
  to be given a rich prior defined by a graphical model. These distributional assumptions
  directly translate into an appropriate aggregation strategy for the context set.
  Moreover, we describe a message-passing procedure that still allows for end-to-end
  optimization with stochastic gradients. We demonstrate the generality of our framework
  by using mixture and Student-t assumptions that yield improvements in function modelling
  and test-time robustness.
openreview: XOQjvkQeoA
title: Exploiting Inferential Structure in Neural Processes
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tailor23a
month: 0
tex_title: Exploiting Inferential Structure in Neural Processes
firstpage: 2089
lastpage: 2098
page: 2089-2098
order: 2089
cycles: false
bibtex_author: Tailor, Dharmesh and Khan, Mohammad Emtiyaz and Nalisnick, Eric
author:
- given: Dharmesh
  family: Tailor
- given: Mohammad Emtiyaz
  family: Khan
- given: Eric
  family: Nalisnick
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/tailor23a/tailor23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/tailor23a/tailor23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
