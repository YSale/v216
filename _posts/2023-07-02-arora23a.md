---
abstract: Pruning deep neural networks is a widely used strategy to alleviate the
  computational burden in machine learning. Overwhelming empirical evidence suggests
  that pruned models retain very high accuracy even with a tiny fraction of parameters.
  However, relatively little work has gone into characterising the small pruned networks
  obtained, beyond a measure of their accuracy. In this paper, we use the sparse double
  descent approach to identify univocally and characterise pruned models associated
  with classification tasks. We observe empirically that, for a given task, iterative
  magnitude pruning (IMP) tends to converge to networks of comparable sizes even when
  starting from full networks with sizes ranging over orders of magnitude. We analyse
  the best pruned models in a controlled experimental setup and show that their number
  of parameters reflects task difficulty and that they are much better than full networks
  at capturing the true conditional probability distribution of the labels. On real
  data, we similarly observe that pruned models are less prone to overconfident predictions.
  Our results suggest that pruned models obtained via IMP not only have advantageous
  computational properties but also provide a better representation of uncertainty
  in learning.
openreview: g3051MQPo4
title: 'Quantifying lottery tickets under label noise: accuracy, calibration, and
  complexity'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arora23a
month: 0
tex_title: 'Quantifying lottery tickets under label noise: accuracy, calibration,
  and complexity'
firstpage: 88
lastpage: 98
page: 88-98
order: 88
cycles: false
bibtex_author: Arora, Viplove and Irto, Daniele and Goldt, Sebastian and Sanguinetti,
  Guido
author:
- given: Viplove
  family: Arora
- given: Daniele
  family: Irto
- given: Sebastian
  family: Goldt
- given: Guido
  family: Sanguinetti
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/arora23a/arora23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/arora23a/arora23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
