---
abstract: Adversarial training (AT) has been considered an imperative component for
  safely deploying neural network-based applications. However, it typically comes
  with slow convergence and worse performance on clean samples (i.e., non-adversarial
  samples). In this work, we analyze the behavior of neural networks during learning
  with adversarial samples through the lens of response frequency. Interestingly,
  we observe that AT causes neural networks to converge slowly to high-frequency information,
  resulting in highly oscillatory predictions near each data point. To learn high-frequency
  content efficiently, we first prove that a universal phenomenon, the frequency principle
  (i.e., lower frequencies are learned first), still holds in AT. Building upon this
  theoretical foundation, we present a novel approach to AT, which we call phase-shifted
  adversarial training (PhaseAT). In PhaseAT, the high-frequency components, which
  are a contributing factor to slow convergence, are adaptively shifted into the low-frequency
  range where faster convergence occurs. For evaluation, we conduct extensive experiments
  on CIFAR-10 and ImageNet, using an adaptive attack that is carefully designed for
  reliable evaluation. Comprehensive results show that PhaseAT substantially improves
  convergence for high-frequency information, thereby leading to improved adversarial
  robustness.
openreview: s9hnx7UNsj
title: Phase-shifted adversarial training
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim23b
month: 0
tex_title: Phase-shifted adversarial training
firstpage: 1068
lastpage: 1077
page: 1068-1077
order: 1068
cycles: false
bibtex_author: Kim, Yeachan and Kim, Seongyeon and Seo, Ihyeok and Shin, Bonggun
author:
- given: Yeachan
  family: Kim
- given: Seongyeon
  family: Kim
- given: Ihyeok
  family: Seo
- given: Bonggun
  family: Shin
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/kim23b/kim23b.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/kim23b/kim23b-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
