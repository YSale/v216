---
abstract: Nash Q-learning may be considered one of the first and most known algorithms
  in multi-agent reinforcement learning (MARL) for learning policies that constitute
  a Nash equilibrium of an underlying general-sum Markov game. Its original proof
  provided asymptotic guarantees and was for the tabular case. Recently, finite-sample
  guarantees have been provided using more modern RL techniques for the tabular case.
  Our work analyzes Nash Q-learning using linear function approximation – a representation
  regime introduced when the state space is large or continuous – and provides finite-sample
  guarantees that indicate its sample efficiency. We find that the obtained performance
  nearly matches an existing efficient result for single-agent RL under the same representation
  and has a polynomial gap when compared to the best-known result for the tabular
  case.
openreview: MQRmY_y3oe1
title: Finite-sample guarantees for Nash Q-learning with linear function approximation
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cisneros-velarde23a
month: 0
tex_title: Finite-sample guarantees for {N}ash {Q}-learning with linear function approximation
firstpage: 424
lastpage: 432
page: 424-432
order: 424
cycles: false
bibtex_author: Cisneros-Velarde, Pedro and Koyejo, Sanmi
author:
- given: Pedro
  family: Cisneros-Velarde
- given: Sanmi
  family: Koyejo
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/cisneros-velarde23a/cisneros-velarde23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/cisneros-velarde23a/cisneros-velarde23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
