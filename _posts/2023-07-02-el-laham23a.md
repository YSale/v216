---
abstract: This work introduces a novel probabilistic deep learning technique called
  deep Gaussian mixture ensembles (DGMEs), which enables accurate quantification of
  both epistemic and aleatoric uncertainty. By assuming the data generating process
  follows that of a Gaussian mixture, DGMEs are capable of approximating complex probability
  distributions, such as heavy-tailed or multimodal distributions. Our contributions
  include the derivation of an expectation-maximization (EM) algorithm used for learning
  the model parameters, which results in an upper-bound on the log-likelihood of training
  data over that of standard deep ensembles.  Additionally, the proposed EM training
  procedure allows for learning of mixture weights, which is not commonly done in
  ensembles. Our experimental results demonstrate that DGMEs outperform state-of-the-art
  uncertainty quantifying deep learning models in handling complex predictive densities.
openreview: HcXj_fX2Ih
title: Deep Gaussian mixture ensembles
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: el-laham23a
month: 0
tex_title: Deep {G}aussian mixture ensembles
firstpage: 549
lastpage: 559
page: 549-559
order: 549
cycles: false
bibtex_author: El-Laham, Yousef and Dalmasso, Niccolo and Fons, Elizabeth and Vyetrenko,
  Svitlana
author:
- given: Yousef
  family: El-Laham
- given: Niccolo
  family: Dalmasso
- given: Elizabeth
  family: Fons
- given: Svitlana
  family: Vyetrenko
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/el-laham23a/el-laham23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/el-laham23a/el-laham23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
