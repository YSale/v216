---
abstract: Not being able to understand and predict the behavior of deep learning systems
  makes it hard to decide what architecture and algorithm to use for a given problem.
  In science and engineering, modeling is a methodology used to understand complex
  systems whose internal processes are opaque. Modeling replaces a complex system
  with a simpler, more interpretable surrogate. Drawing inspiration from this, we
  construct a class of surrogate models for neural networks using Gaussian processes.
  Rather than deriving kernels for infinite neural networks, we learn kernels empirically
  from the naturalistic behavior of finite neural networks. We demonstrate our approach
  captures existing phenomena related to the spectral bias of neural networks, and
  then show that our surrogate models can be used to solve practical problems such
  as identifying which points most influence the behavior of specific neural networks
  and predicting which architectures and algorithms will generalize well for specific
  datasets.
openreview: TjSW-M4t8
title: Gaussian Process Surrogate Models for Neural Networks
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li23c
month: 0
tex_title: "{G}aussian Process Surrogate Models for Neural Networks"
firstpage: 1241
lastpage: 1252
page: 1241-1252
order: 1241
cycles: false
bibtex_author: Li, Michael Y. and Grant, Erin and Griffiths, Thomas L.
author:
- given: Michael Y.
  family: Li
- given: Erin
  family: Grant
- given: Thomas L.
  family: Griffiths
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/li23c/li23c.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/li23c/li23c-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
