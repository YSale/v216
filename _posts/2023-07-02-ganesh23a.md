---
abstract: Stochastic Heavy Ball (SHB) and Nesterovâ€™s Accelerated Stochastic Gradient
  (ASG) are popular momentum methods in optimization. While the benefits of these
  acceleration ideas in deterministic settings are well understood, their advantages
  in stochastic optimization are unclear. Several works have recently claimed that
  SHB and ASG always help in stochastic optimization. Our work shows that i.) these
  claims are either flawed or one-sided (e.g., consider only the bias term but not
  the variance), and ii.) when both these terms are accounted for, SHB and ASG do
  not always help. Specifically, for <em>any</em> quadratic optimization, we obtain
  a lower bound on the sample complexity of SHB and ASG, accounting for both bias
  and variance, and show that the vanilla SGD can achieve the same bound.
openreview: H4M66ZDxkSy
title: Does Momentum Help in Stochastic Optimization? A Sample Complexity Analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ganesh23a
month: 0
tex_title: Does Momentum Help in Stochastic Optimization? {A} Sample Complexity Analysis.
firstpage: 602
lastpage: 612
page: 602-612
order: 602
cycles: false
bibtex_author: Ganesh, Swetha and Deb, Rohan and Thoppe, Gugan and Budhiraja, Amarjit
author:
- given: Swetha
  family: Ganesh
- given: Rohan
  family: Deb
- given: Gugan
  family: Thoppe
- given: Amarjit
  family: Budhiraja
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/ganesh23a/ganesh23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/ganesh23a/ganesh23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
