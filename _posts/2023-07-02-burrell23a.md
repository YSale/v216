---
abstract: Do common assumptions about the way that crowd workers make mistakes in
  microtask (labeling) applications manifest in real crowdsourcing data? Prior work
  only addresses this question indirectly. Instead, it primarily focuses on designing
  new label aggregation algorithms, seeming to imply that better performance justifies
  any additional assumptions. However, empirical evidence in past instances has raised
  significant challenges to common assumptions. We continue this line of work, using
  crowdsourcing data itself as directly as possible to interrogate several basic assumptions
  about workers and tasks. We find strong evidence that the assumption that workers
  respond correctly to each task with a constant probability, which is common in theoretical
  work, is implausible in real data. We also illustrate how heterogeneity among tasks
  and workers can take different forms, which have different implications for the
  design and evaluation of label aggregation algorithms.
openreview: AqoPK_uL-_z
software: https://github.com/burrelln/Testing-Conventional-Wisdom
title: Testing conventional wisdom (of the crowd)
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: burrell23a
month: 0
tex_title: Testing conventional wisdom (of the crowd)
firstpage: 237
lastpage: 248
page: 237-248
order: 237
cycles: false
bibtex_author: Burrell, Noah and Schoenebeck, Grant
author:
- given: Noah
  family: Burrell
- given: Grant
  family: Schoenebeck
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/burrell23a/burrell23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/burrell23a/burrell23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
