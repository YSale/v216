---
abstract: Efficiently and reliably estimating uncertainty is an important objective
  in deep learning. It is especially pertinent to autoregressive sequence tasks, where
  training and inference costs are typically very high. However, existing research
  has predominantly focused on tasks with static data such as image classification.
  In this work, we investigate Ensemble Distribution Distillation (EDD) applied to
  large-scale natural language sequence-to-sequence data. EDD aims to compress the
  superior uncertainty performance of an expensive (teacher) ensemble into a cheaper
  (student) single model. Importantly, the ability to separate knowledge (epistemic)
  and data (aleatoric) uncertainty is retained. Existing probability-space approaches
  to EDD, however, are difficult to scale to large vocabularies. We show, for modern
  transformer architectures on large-scale translation tasks, that modelling the ensemble
  <em>logits</em>, instead of softmax probabilities, leads to significantly better
  students. Moreover, the students surprisingly even <em>outperform Deep Ensembles</em>
  by up to $\sim$10% AUROC on out-of-distribution detection, whilst matching them
  at in-distribution translation.
openreview: KydoGh5ynl
title: Logit-based ensemble distribution distillation for robust autoregressive sequence
  uncertainties
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fathullah23a
month: 0
tex_title: Logit-based ensemble distribution distillation for robust autoregressive
  sequence uncertainties
firstpage: 582
lastpage: 591
page: 582-591
order: 582
cycles: false
bibtex_author: Fathullah, Yassir and Xia, Guoxuan and J. F. Gales, Mark
author:
- given: Yassir
  family: Fathullah
- given: Guoxuan
  family: Xia
- given: Mark
  family: J. F. Gales
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/fathullah23a/fathullah23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/fathullah23a/fathullah23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
