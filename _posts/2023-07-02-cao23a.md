---
abstract: Despite the advances in Visual Question Answering (VQA), many VQA models
  currently suffer from language priors (i.e. generating answers directly from questions
  without using images), which severely reduces their robustness in real-world scenarios.
  We propose a novel training strategy called Loss Rebalancing Label and Global Context
  (LRLGC) to alleviate the above problem. Specifically, the Loss Rebalancing Label
  (LRL) is dynamically constructed based on the degree of sample bias to accurately
  adjust losses across samples and ensure a more balanced form of total losses in
  VQA. In addition, the Global Context (GC) provides the model with valid global information
  to assist the model in predicting answers more accurately. Finally, the model is
  trained through an ensemble-based approach that retains the beneficial effects of
  biased samples on the model while reducing their importance. Our approach is model-agnostic
  and enables end-to-end training. Extensive experimental results show that LRLGC
  (1) improves performance for various VQA models and (2) performs competitively in
  the VQA-CP v2 benchmark test.
openreview: FEDih5lICO
title: Overcoming Language Priors for Visual Question Answering via Loss Rebalancing
  Label and Global Context
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cao23a
month: 0
tex_title: Overcoming Language Priors for Visual Question Answering via Loss Rebalancing
  Label and Global Context
firstpage: 249
lastpage: 259
page: 249-259
order: 249
cycles: false
bibtex_author: Cao, Runlin and Li, Zhixin
author:
- given: Runlin
  family: Cao
- given: Zhixin
  family: Li
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/cao23a/cao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
