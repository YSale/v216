---
abstract: Multi-model Markov decision process (MMDP) is a promising framework for
  computing policies that are robust to parameter uncertainty in MDPs. MMDPs aim to
  find a policy that maximizes the expected return over a distribution of MDP models.
  Because MMDPs are NP-hard to solve, most methods resort to approximations. In this
  paper, we derive the policy gradient of MMDPs and propose CADP, which combines a
  coordinate ascent method and a dynamic programming algorithm for solving MMDPs.
  The main innovation of CADP compared with earlier algorithms is to take the coordinate
  ascent perspective to adjust model weights iteratively to guarantee monotone policy
  improvements to a local maximum. A theoretical analysis of CADP proves that it never
  performs worse than previous dynamic programming algorithms like WSU. Our numerical
  results indicate that CADP substantially outperforms existing methods on several
  benchmark problems.
openreview: RW36qtv1F-5
software: https://github.com/suxh2019/CADP
title: Solving multi-model MDPs by coordinate ascent and dynamic programming
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: su23a
month: 0
tex_title: Solving multi-model {MDPs} by coordinate ascent and dynamic programming
firstpage: 2016
lastpage: 2025
page: 2016-2025
order: 2016
cycles: false
bibtex_author: Su, Xihong and Petrik, Marek
author:
- given: Xihong
  family: Su
- given: Marek
  family: Petrik
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/su23a/su23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/su23a/su23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
