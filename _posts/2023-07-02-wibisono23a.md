---
abstract: 'Bidirectional attention - composed of the neural network architecture of
  self-attention with positional encodings, together with the masked language model
  (MLM) objective - has emerged as a key component of modern large language models
  (LLMs). Despite its empirical success, few studies have examined its statistical
  underpinnings: What statistical model is bidirectional attention implicitly fitting?
  What sets it apart from its non-attention predecessors? We explore these questions
  in this paper. The key observation is that fitting a single-layer single-head bidirectional
  attention, upon reparameterization, is equivalent to fitting a continuous bag of
  words (CBOW) model with mixture-of-experts (MoE) weights. Further, bidirectional
  attention with multiple heads and multiple layers is equivalent to stacked MoEs
  and a mixture of MoEs, respectively. This statistical viewpoint reveals the distinct
  use of MoE in bidirectional attention, which aligns with its practical effectiveness
  in handling heterogeneous data. It also suggests an immediate extension to categorical
  tabular data, if we view each word location in a sentence as a tabular feature.
  Across empirical studies, we find that this extension outperforms existing tabular
  extensions of transformers in out-of-distribution (OOD) generalization. Finally,
  this statistical perspective of bidirectional attention enables us to theoretically
  characterize when linear word analogies are present in its word embeddings. These
  analyses show that bidirectional attention can require much stronger assumptions
  to exhibit linear word analogies than its non-attention predecessors.'
openreview: FLle97UXEr
title: Bidirectional Attention as a Mixture of Continuous Word Experts
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wibisono23a
month: 0
tex_title: Bidirectional Attention as a Mixture of Continuous Word Experts
firstpage: 2271
lastpage: 2281
page: 2271-2281
order: 2271
cycles: false
bibtex_author: Wibisono, Kevin C. and Wang, Yixin
author:
- given: Kevin C.
  family: Wibisono
- given: Yixin
  family: Wang
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/wibisono23a/wibisono23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/wibisono23a/wibisono23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
