---
abstract: The vision transformer has achieved state-of-the-art performance in various
  vision tasks; however, the memory consumption is larger than those of previous convolutional
  neural network based models because of $O(N^2)$ time and memory complexity of the
  general self-attention models. Many approaches aim to change the complexity to $O(N)$
  to solve this problem; however, they stack deep convolutional layers to retain locality
  or complicate the architecture as seen in window attention, to compensate for the
  performance degradation. To solve these problems, we propose ViBid algorithm, which
  resolves the complexity problem of $O(N^2)$ by replacing Softmax with bidirectional
  normalization (BiNorm). In addition, it has a much simpler architecture than the
  existing transformer model with $O(N)$ complexity. Owing to our simple architecture,
  we were able to use larger resolutions for training, and we obtained a lighter and
  superior GPU throughput model with competitive performance. ViBid can be used with
  any transformer method that uses queries, keys, and values (QKV) because of BiNorm,
  and it is quite universal due to its simple architectural structure.
openreview: zShXcBWXet
title: 'ViBid: Linear Vision Transformer with Bidirectional Normalization'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: song23a
month: 0
tex_title: "{ViBid}: Linear Vision Transformer with Bidirectional Normalization"
firstpage: 1996
lastpage: 2005
page: 1996-2005
order: 1996
cycles: false
bibtex_author: Song, Jeonggeun and Lee, Heung-Chang
author:
- given: Jeonggeun
  family: Song
- given: Heung-Chang
  family: Lee
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/song23a/song23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/song23a/song23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
