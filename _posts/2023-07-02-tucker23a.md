---
abstract: Many machine learning applications rely on large datasets that are conveniently
  collected from existing sources or that are labeled automatically as a by-product
  of user actions. However, in settings such as content moderation, accurately and
  reliably labeled data comes at substantial cost. If a learning algorithm has to
  pay for reward information, for example by asking a human for feedback, how does
  this change the exploration/exploitation tradeoff? We study this question in the
  context of bandit learning. Specifically, we investigate Bandits with Costly Reward
  Observations, where a cost needs to be paid in order to observe the reward of the
  banditâ€™s action. We show that the observation cost implies an $\Omega(c^{1/3}T^{2/3})$
  lower bound on the regret. Furthermore, we develop a general non-adaptive bandit
  algorithm which matches this lower bound, and we present several competitive adaptive
  learning algorithms for both k-armed and contextual bandits.
openreview: pUm5S32Q1p
title: Bandits with costly reward observations
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tucker23a
month: 0
tex_title: Bandits with costly reward observations
firstpage: 2147
lastpage: 2156
page: 2147-2156
order: 2147
cycles: false
bibtex_author: Tucker, Aaron D. and Biddulph, Caleb and Wang, Claire and Joachims,
  Thorsten
author:
- given: Aaron D.
  family: Tucker
- given: Caleb
  family: Biddulph
- given: Claire
  family: Wang
- given: Thorsten
  family: Joachims
date: 2023-07-02
address:
container-title: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
  Intelligence
volume: '216'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 2
pdf: https://proceedings.mlr.press/v216/tucker23a/tucker23a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v216/tucker23a/tucker23a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
